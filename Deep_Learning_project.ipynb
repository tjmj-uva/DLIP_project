{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 18613,
          "sourceType": "datasetVersion",
          "datasetId": 5839
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Deep Learning project",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjmj-uva/DLIP_project/blob/main/Deep_Learning_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "nih_chest_xrays_data_path = kagglehub.dataset_download('khanfashee/nih-chest-x-ray-14-224x224-resized')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsvdHfgrjWDq",
        "outputId": "63ce3668-5dd3-4450-f025-4abddd988564"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/khanfashee/nih-chest-x-ray-14-224x224-resized?dataset_version_number=3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.30G/2.30G [01:47<00:00, 22.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "Iu_4abISjWDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "look at post kaggle competition predicting\n",
        "\n",
        "Proven in medical imaging: Many studies show EfficientNet outperforms older CNNs like DenseNet-121 on X-ray, CT, and MRI datasets.\n",
        "\n",
        "In this project we will build upon on the DannyNet framework to imporve the classification of diffrenet strains of pneumonia using chest X-rays. DannyNet has demonstrated strong performance using DensNet - 121 we aim to enhance it's accuracy as it is based on some older techniques.\n",
        "\n",
        "**Data**\n",
        "We will use the NIH ChestX-ray large dataset of 112,120 x-ray images taken from 30,805 patients. It has 14 labels and some of the x-rays are labeled with more than one diagnosis.\n",
        "https://www.kaggle.com/datasets/nih-chest-xrays/data/data?select=README_CHESTXRAY.pdf\n",
        "\n",
        "we wanted to better Dannynet and instead of using densenet we would use **efficient netB0/B4**, this is more efficient as it uses less weights and it shows that there is a higher accuracy when used on medical images(source)\n",
        "\n",
        "We can add **CBAM** which add two types of attention, **channel attention** where it see which featires matter and **spatial attention** which allows the network to focus on a specific part of the image as it might be more informative.\n",
        "\n",
        "**To imporve robustness**\n",
        "\n",
        "We would better **data augmentation** (adds to DennyNet)\n",
        "\n",
        "* geometric augmentations\n",
        "* RandomResizedCrop(224)\n",
        "* RandomHorizontalFlip\n",
        "* RandomRotation(5°)\n",
        "\n",
        "**Intensity augmentations**\n",
        "\n",
        "* ColorJitter(brightness=0.1, contrast=0.1)\n",
        "\n",
        "* RandomGamma → changes exposure\n",
        "\n",
        "* RandomContrast → simulates different machines\n",
        "\n",
        "**mixing augmentations (Dennynet doesnt have)**\n",
        "\n",
        "* Mixup → blends two images + labels\n",
        "\n",
        "**Better per class threshold tuning???**\n",
        "what is the best threshold to differentiate classes -> uses different threshold for each class\n",
        "\n",
        "**better preprocessing**\n",
        "* Resize consistently (224×224 or 256×256).\n",
        "* Normalize with ImageNet stats.\n",
        "\n",
        "We use **Focal loss and AdamW** similarly as they do in densetent -> address class imbalance and provide stable, efficient training.\n",
        "\n",
        "Then we will compare it with the original DensNet-121 and human predictions if they are any or otherwise with a Bayesian Baseline Gaussian Naive becuase it is quick and simple but usually very accurate.\n",
        "\n",
        "**We will copmare it based on:**\n",
        "\n",
        "* Accuracy\n",
        "* AUROC (Area Under the ROC Curve) → measures ranking ability\n",
        "* F1 score → measures exact yes/no detection\n",
        "\n",
        "**still need to add training details**\n",
        "* Batch size\n",
        "* Number of epochs\n",
        "* Learning rate"
      ],
      "metadata": {
        "id": "Wgb9bELDjWDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = nih_chest_xrays_data_path\n",
        "print(f\"Data directory: {data_dir}\")\n",
        "print(os.listdir(data_dir))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-07T18:55:06.48899Z",
          "iopub.execute_input": "2025-12-07T18:55:06.489217Z",
          "iopub.status.idle": "2025-12-07T18:55:06.506914Z",
          "shell.execute_reply.started": "2025-12-07T18:55:06.489196Z",
          "shell.execute_reply": "2025-12-07T18:55:06.505981Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdvb1XXTjWDw",
        "outputId": "e9a370ee-ebf9-4c18-985d-e616da8d8aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data directory: /root/.cache/kagglehub/datasets/khanfashee/nih-chest-x-ray-14-224x224-resized/versions/3\n",
            "['train_val_list_NIH.txt', 'Data_Entry_2017.csv', 'test_list_NIH.txt', 'images-224', 'BBox_List_2017_Official_NIH.csv', 'pretrained_model.h5']\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_curve\n",
        "import numpy as np\n",
        "from torchvision.models import densenet121, DenseNet121_Weights\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "CONFIG = {\n",
        "    \"model\": \"dannynet\",\n",
        "    \"batch_size\": 8, #8\n",
        "    \"learning_rate\": 0.00005,\n",
        "    \"epochs\": 1, #25\n",
        "    \"num_workers\": 2, #2\n",
        "    \"device\": \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    \"data_dir\": \"/root/.cache/kagglehub/datasets/khanfashee/nih-chest-x-ray-14-224x224-resized/versions/3\",\n",
        "    \"patience\": 5, #5\n",
        "    \"seed\": 42,\n",
        "    \"image_size\": 224,\n",
        "}\n",
        "\n",
        "# Define image transformations (consistent with CheXNet)\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        " # Load and modify the model\n",
        "model = densenet121(weights=DenseNet121_Weights.IMAGENET1K_V1)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 14)\n",
        "model = model.to(CONFIG[\"device\"])\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        bce_loss = self.bce(inputs, targets)\n",
        "        pt = torch.exp(-bce_loss)\n",
        "        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss\n",
        "        if self.reduction == 'mean':\n",
        "            return focal_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return focal_loss.sum()\n",
        "        else:\n",
        "            return focal_loss\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = FocalLoss(alpha=1, gamma=2)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=1e-5) #Added weight decay. # betas=(0.9, 0.999) - this is default in pytorch\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.1)\n",
        "\n",
        "\n",
        "data_path = CONFIG[\"data_dir\"]\n",
        "csv_file = os.path.join(data_path, \"Data_Entry_2017.csv\")\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Get list of all image folders from images_001 to images_012\n",
        "\n",
        "# Step-by-step debugging\n",
        "images_folder = os.path.join(data_path, \"images-224\", \"images-224\")\n",
        "\n",
        "print(f\"Looking for images in: {images_folder}\")\n",
        "print(f\"Folder exists: {os.path.exists(images_folder)}\")\n",
        "\n",
        "# Build the mapping\n",
        "image_to_folder = {}\n",
        "if os.path.exists(images_folder):\n",
        "    all_files = os.listdir(images_folder)\n",
        "    png_files = [f for f in all_files if f.endswith('.png')]\n",
        "\n",
        "    print(f\"Total files in folder: {len(all_files)}\")\n",
        "    print(f\"PNG files found: {len(png_files)}\")\n",
        "    print(f\"Sample PNG files: {png_files[:5]}\")\n",
        "\n",
        "    for img_file in png_files:\n",
        "        image_to_folder[img_file] = images_folder\n",
        "else:\n",
        "    print(\"ERROR: Folder does not exist!\")\n",
        "\n",
        "print(f\"\\nTotal images mapped: {len(image_to_folder)}\")\n",
        "\n",
        "# Check CSV\n",
        "print(f\"\\nCSV before filtering: {len(df)} rows\")\n",
        "print(f\"Sample Image Index from CSV: {df['Image Index'].head().tolist()}\")\n",
        "\n",
        "# Filter\n",
        "df = df[df['Image Index'].isin(image_to_folder.keys())]\n",
        "\n",
        "print(f\"\\nCSV after filtering: {len(df)} rows\")\n",
        "print(f\"Unique patients: {df['Patient ID'].nunique()}\")\n",
        "\n",
        "# Unique patient IDs\n",
        "unique_patients = df['Patient ID'].unique()\n",
        "\n",
        "# Split patients — not rows\n",
        "train_val_patients, test_patients = train_test_split(\n",
        "unique_patients, test_size=0.02, random_state=CONFIG[\"seed\"]\n",
        ")\n",
        "\n",
        "train_patients, val_patients = train_test_split(\n",
        "train_val_patients, test_size=0.052, random_state=CONFIG[\"seed\"]\n",
        ")\n",
        "\n",
        "#Use those patients to filter full image rows\n",
        "train_df = df[df['Patient ID'].isin(train_patients)]\n",
        "val_df   = df[df['Patient ID'].isin(val_patients)]\n",
        "test_df  = df[df['Patient ID'].isin(test_patients)]\n",
        "\n",
        "\n",
        "# List of diseases we’re classifying\n",
        "disease_list = [\n",
        "    'Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion',\n",
        "    'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass',\n",
        "    'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'\n",
        "]\n",
        "\n",
        "# Function to convert label string to a vector\n",
        "def get_label_vector(labels_str):\n",
        "    labels = labels_str.split('|')\n",
        "\n",
        "    if labels == ['No Finding']:\n",
        "        return [0] * len(disease_list)\n",
        "\n",
        "    else:\n",
        "        return [1 if disease in labels else 0 for disease in disease_list]\n",
        "\n",
        "# Custom Dataset class\n",
        "class CheXNetDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_to_folder, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_to_folder = image_to_folder\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataframe.iloc[idx]['Image Index']\n",
        "        folder = self.image_to_folder[img_name]\n",
        "\n",
        "        img_path = os.path.join(folder, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        labels_str = self.dataframe.iloc[idx]['Finding Labels']\n",
        "        label_vector = get_label_vector(labels_str)\n",
        "        labels = torch.tensor(label_vector, dtype=torch.float)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "# Set up DataLoaders with our custom datasets\n",
        "train_dataset = CheXNetDataset(train_df, image_to_folder, transform=transform_train)\n",
        "val_dataset = CheXNetDataset(val_df, image_to_folder, transform=transform_test)\n",
        "test_dataset = CheXNetDataset(test_df, image_to_folder, transform=transform_test)\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, num_workers=CONFIG[\"num_workers\"])\n",
        "valloader = DataLoader(val_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "testloader = DataLoader(test_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
        "\n",
        "\n",
        "def get_optimal_thresholds(labels, preds):\n",
        "    thresholds = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        precision, recall, thresh = precision_recall_curve(labels[:, i], preds[:, i])\n",
        "        f1_scores = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "        best_threshold = thresh[np.argmax(f1_scores)] if len(thresh) > 0 else 0.5\n",
        "        thresholds.append(best_threshold)\n",
        "    return thresholds\n",
        "\n",
        "def evaluate(model, loader, criterion, device, desc=\"[Test]\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels, all_preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(loader, desc=desc):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "            preds = torch.sigmoid(outputs)\n",
        "            all_labels.append(labels.cpu())\n",
        "            all_preds.append(preds.cpu())\n",
        "\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    thresholds = get_optimal_thresholds(all_labels, all_preds)\n",
        "\n",
        "    preds_binary = np.zeros_like(all_preds)\n",
        "    for i in range(all_preds.shape[1]):\n",
        "        preds_binary[:, i] = (all_preds[:, i] > thresholds[i]).astype(int)\n",
        "\n",
        "    auc_scores = [roc_auc_score(all_labels[:, i], all_preds[:, i]) for i in range(14)]\n",
        "    f1_scores = [f1_score(all_labels[:, i], preds_binary[:, i]) for i in range(14)]\n",
        "\n",
        "    avg_auc = np.mean(auc_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    for i, disease in enumerate(disease_list):\n",
        "        print(f\"{desc} {disease} AUC: {auc_scores[i]:.4f} | F1: {f1_scores[i]:.4f}\")\n",
        "\n",
        "    print(f\"{desc} Avg AUC: {avg_auc:.4f}, Avg F1: {avg_f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"loss\": running_loss / len(loader),\n",
        "        \"avg_auc\": avg_auc,\n",
        "        \"avg_f1\": avg_f1,\n",
        "        \"auc_dict\": dict(zip(disease_list, auc_scores)),\n",
        "        \"f1_dict\": dict(zip(disease_list, f1_scores)),\n",
        "        \"thresholds\": dict(zip(disease_list, thresholds))\n",
        "    }\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train(epoch, model, trainloader, optimizer, criterion, CONFIG):\n",
        "    device = CONFIG[\"device\"]\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    progress_bar = tqdm(trainloader, desc=f\"Epoch {epoch+1}/{CONFIG['epochs']} [Train]\", leave=True)\n",
        "    for i, (inputs, labels) in enumerate(progress_bar):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"loss\": running_loss / (i + 1)})\n",
        "    train_loss = running_loss / len(trainloader)\n",
        "    return train_loss\n",
        "\n",
        "def validate(model, valloader, criterion, device):\n",
        "    return evaluate(model, valloader, criterion, device, desc=\"[Validate]\")\n",
        "\n",
        " #  Initialize TensorBoard\n",
        "timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "run_id = f\"chexnet_{timestamp}\"\n",
        "writer = SummaryWriter(log_dir=f'runs/{run_id}')\n",
        "\n",
        "# Log config as text\n",
        "config_str = \"\\n\".join([f\"{k}: {v}\" for k, v in CONFIG.items()])\n",
        "writer.add_text('Config', config_str)\n",
        "\n",
        "checkpoint_dir = os.path.join(\"models\", run_id)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "best_val_auc = 0.0\n",
        "patience_counter = 0\n",
        "\n",
        "\n",
        "for epoch in range(CONFIG[\"epochs\"]):\n",
        "    train_loss = train(epoch, model, trainloader, optimizer, criterion, CONFIG)\n",
        "    val_stats = validate(model, valloader, criterion, CONFIG[\"device\"])\n",
        "    scheduler.step(val_stats[\"loss\"])\n",
        "\n",
        "    # Log to TensorBoard\n",
        "    writer.add_scalar('Loss/train', train_loss, epoch + 1)\n",
        "    writer.add_scalar('Loss/val', val_stats[\"loss\"], epoch + 1)\n",
        "    writer.add_scalar('AUC/val', val_stats[\"avg_auc\"], epoch + 1)\n",
        "    writer.add_scalar('F1/val', val_stats[\"avg_f1\"], epoch + 1)\n",
        "\n",
        "    # Log per-disease metrics\n",
        "    for disease, auc in val_stats[\"auc_dict\"].items():\n",
        "        writer.add_scalar(f'AUC_per_disease/{disease}', auc, epoch + 1)\n",
        "    for disease, f1 in val_stats[\"f1_dict\"].items():\n",
        "        writer.add_scalar(f'F1_per_disease/{disease}', f1, epoch + 1)\n",
        "\n",
        "    if val_stats[\"avg_auc\"] > best_val_auc:\n",
        "        best_val_auc = val_stats[\"avg_auc\"]\n",
        "\n",
        "        patience_counter = 0\n",
        "        timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"best_model_{timestamp}.pth\")\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= CONFIG[\"patience\"]:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "# Evaluate the best model\n",
        "best_checkpoint_path = sorted([os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.startswith('best_model_')])[-1]\n",
        "model.load_state_dict(torch.load(best_checkpoint_path))\n",
        "test_stats = evaluate(model, testloader, criterion, CONFIG[\"device\"])\n",
        "\n",
        "# Log test results to TensorBoard\n",
        "writer.add_scalar('Test/loss', test_stats[\"loss\"], 0)\n",
        "writer.add_scalar('Test/auc', test_stats[\"avg_auc\"], 0)\n",
        "writer.add_scalar('Test/f1', test_stats[\"avg_f1\"], 0)\n",
        "\n",
        "for disease, auc in test_stats[\"auc_dict\"].items():\n",
        "    writer.add_scalar(f'Test_AUC/{disease}', auc, 0)\n",
        "for disease, f1 in test_stats[\"f1_dict\"].items():\n",
        "    writer.add_scalar(f'Test_F1/{disease}', f1, 0)\n",
        "\n",
        "writer.close()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-08T01:18:45.577063Z",
          "iopub.execute_input": "2025-12-08T01:18:45.577374Z",
          "iopub.status.idle": "2025-12-08T01:21:31.077493Z",
          "shell.execute_reply.started": "2025-12-08T01:18:45.577339Z",
          "shell.execute_reply": "2025-12-08T01:21:31.07597Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvXTtrQvjWDx",
        "outputId": "a6978efa-0e5c-494e-c3a3-53be05da1ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for images in: /root/.cache/kagglehub/datasets/khanfashee/nih-chest-x-ray-14-224x224-resized/versions/3/images-224/images-224\n",
            "Folder exists: True\n",
            "Total files in folder: 112120\n",
            "PNG files found: 112120\n",
            "Sample PNG files: ['00023179_006.png', '00017698_008.png', '00016220_021.png', '00013141_000.png', '00010825_001.png']\n",
            "\n",
            "Total images mapped: 112120\n",
            "\n",
            "CSV before filtering: 112120 rows\n",
            "Sample Image Index from CSV: ['00000001_000.png', '00000001_001.png', '00000001_002.png', '00000002_000.png', '00000003_000.png']\n",
            "\n",
            "CSV after filtering: 112120 rows\n",
            "Unique patients: 30805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 [Train]: 100%|██████████| 12981/12981 [25:07<00:00,  8.61it/s, loss=0.0471]\n",
            "[Validate]: 100%|██████████| 747/747 [00:29<00:00, 25.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Validate] Atelectasis AUC: 0.7909 | F1: 0.3597\n",
            "[Validate] Cardiomegaly AUC: 0.8510 | F1: 0.2997\n",
            "[Validate] Consolidation AUC: 0.7934 | F1: 0.2161\n",
            "[Validate] Edema AUC: 0.8481 | F1: 0.1732\n",
            "[Validate] Effusion AUC: 0.8419 | F1: 0.4841\n",
            "[Validate] Emphysema AUC: 0.8690 | F1: 0.3542\n",
            "[Validate] Fibrosis AUC: 0.8118 | F1: 0.1358\n",
            "[Validate] Hernia AUC: 0.8498 | F1: 0.1081\n",
            "[Validate] Infiltration AUC: 0.6965 | F1: 0.4118\n",
            "[Validate] Mass AUC: 0.8087 | F1: 0.3292\n",
            "[Validate] Nodule AUC: 0.7274 | F1: 0.2507\n",
            "[Validate] Pleural_Thickening AUC: 0.7641 | F1: 0.1933\n",
            "[Validate] Pneumonia AUC: 0.6426 | F1: 0.0451\n",
            "[Validate] Pneumothorax AUC: 0.8439 | F1: 0.3668\n",
            "[Validate] Avg AUC: 0.7956, Avg F1: 0.2663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Test]: 100%|██████████| 288/288 [00:11<00:00, 24.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Test] Atelectasis AUC: 0.7628 | F1: 0.3341\n",
            "[Test] Cardiomegaly AUC: 0.8786 | F1: 0.3511\n",
            "[Test] Consolidation AUC: 0.7518 | F1: 0.2047\n",
            "[Test] Edema AUC: 0.8506 | F1: 0.2703\n",
            "[Test] Effusion AUC: 0.8728 | F1: 0.5571\n",
            "[Test] Emphysema AUC: 0.8983 | F1: 0.3537\n",
            "[Test] Fibrosis AUC: 0.7623 | F1: 0.0960\n",
            "[Test] Hernia AUC: 0.9081 | F1: 0.3571\n",
            "[Test] Infiltration AUC: 0.6821 | F1: 0.3834\n",
            "[Test] Mass AUC: 0.8454 | F1: 0.3789\n",
            "[Test] Nodule AUC: 0.6763 | F1: 0.2286\n",
            "[Test] Pleural_Thickening AUC: 0.7379 | F1: 0.2432\n",
            "[Test] Pneumonia AUC: 0.7012 | F1: 0.0465\n",
            "[Test] Pneumothorax AUC: 0.8497 | F1: 0.2788\n",
            "[Test] Avg AUC: 0.7984, Avg F1: 0.2917\n"
          ]
        }
      ],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods"
      ],
      "metadata": {
        "id": "LsBcw-IijWD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data augmentation"
      ],
      "metadata": {
        "id": "l6qD6Xu6jWD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data preprocessing"
      ],
      "metadata": {
        "id": "4Tn5BLntjWD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Models"
      ],
      "metadata": {
        "id": "nf_amQe3jWD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Baseline: GaussianNB"
      ],
      "metadata": {
        "id": "AIs2n1LyjWD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet - B4 + CBAM + 14 class threshold (maybe)"
      ],
      "metadata": {
        "id": "nLNKFGcljWD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function or a key product"
      ],
      "metadata": {
        "id": "1S6QokUYjWD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross entropy loss"
      ],
      "metadata": {
        "id": "Cs_XE8hdjWD9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Focal loss -> focuses on hard to classify or rare examples"
      ],
      "metadata": {
        "id": "Jzhz0o94jWD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fitting"
      ],
      "metadata": {
        "id": "aB7TpfiljWD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "H2GSqX_JjWD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance metric"
      ],
      "metadata": {
        "id": "2pmtUmGBjWEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### accuracy"
      ],
      "metadata": {
        "id": "UuBKertBjWEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AUC"
      ],
      "metadata": {
        "id": "QG8v5n5ajWEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F1 ->  multi-label classification and datasets with high class imbalance"
      ],
      "metadata": {
        "id": "WuNRjFXgjWEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison"
      ],
      "metadata": {
        "id": "kzAKG0rcjWEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model insight and robustness analysis (classification error, inputs that are problematic)"
      ],
      "metadata": {
        "id": "qs_mYSlSjWED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confidence"
      ],
      "metadata": {
        "id": "f5JB2WjSjWED"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "D8Q3jmJxjWEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generalizability"
      ],
      "metadata": {
        "id": "YoIUZT3bjWEE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improvements"
      ],
      "metadata": {
        "id": "_FbqvPndjWEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## take away for practitioners and for researchers"
      ],
      "metadata": {
        "id": "uvlyRTSgjWEF"
      }
    }
  ]
}