{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 749924,
          "sourceType": "datasetVersion",
          "datasetId": 388321
        },
        {
          "sourceId": 9539774,
          "sourceType": "datasetVersion",
          "datasetId": 5811128
        }
      ],
      "dockerImageVersionId": 31193,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "history_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tjmj-uva/DLIP_project/blob/main/EfficientNet%20-%20replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jbeltranleon_nih_chest_xrays_224_gray_path = kagglehub.dataset_download('jbeltranleon/nih-chest-xrays-224-gray')\n",
        "vinu210110b_data_entry_2017_v2020_path = kagglehub.dataset_download('vinu210110b/data-entry-2017-v2020')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sc5WiiFKsl3",
        "outputId": "fcfc7f81-520a-416d-f6cf-7bdd16e1ef8a"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'nih-chest-xrays-224-gray' dataset.\n",
            "Using Colab cache for faster access to the 'data-entry-2017-v2020' dataset.\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from glob import glob\n",
        "from itertools import chain\n",
        "from contextlib import redirect_stdout\n",
        "import shutil\n",
        "import warnings\n",
        "import json\n",
        "\n",
        "# TensorFlow 2.x / Keras imports\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.applications import EfficientNetB0, EfficientNetB1, EfficientNetB2, EfficientNetB3, EfficientNetB5\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow.keras.backend as kb\n",
        "\n",
        "# Kaggle automatically mounts datasets at /kaggle/input/\n",
        "inpath = jbeltranleon_nih_chest_xrays_224_gray_path + \"/\"\n",
        "print(\"Dataset contents:\")\n",
        "print(os.listdir(inpath))\n",
        "\n",
        "# Load data\n",
        "data = pd.read_csv(os.path.join(vinu210110b_data_entry_2017_v2020_path, 'Data_Entry_2017_v2020.csv'))\n",
        "print(f\"shape : {data.shape}\")\n",
        "\n",
        "data.head()\n",
        "\n",
        "data = data[data['Patient Age']<100]\n",
        "\n",
        "print(f\" shape : {data.shape}\")\n",
        "\n",
        "data = data[['Image Index', 'Finding Labels']]\n",
        "\n",
        "print(f\" shape: {data.shape}\")\n",
        "\n",
        "# Map image paths\n",
        "all_image_paths = {os.path.basename(x): x for x in\n",
        "                   glob(os.path.join(inpath, '**/*.png'), recursive=True)}\n",
        "print('Imágenes encontradas:', len(all_image_paths))\n",
        "\n",
        "data['Path'] = data['Image Index'].map(all_image_paths.get)\n",
        "\n",
        "# Remove rows where image path is missing\n",
        "data = data.dropna(subset=['Path'])\n",
        "print(f\"After filtering missing images: {data.shape}\")\n",
        "\n",
        "data.sample(5, random_state=3)\n",
        "\n",
        "\n",
        "# Extract unique labels\n",
        "all_labels = np.unique(list(chain(*data['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
        "\n",
        "all_labels\n",
        "\n",
        "all_labels = np.delete(all_labels, np.where(all_labels == 'No Finding'))\n",
        "print(f'Tipo actual: {type(all_labels)}')\n",
        "\n",
        "all_labels = [x for x in all_labels]\n",
        "print(f'Tipo final: {type(all_labels)}')\n",
        "\n",
        "print(f'Enfermedades: ({len(all_labels)}): {all_labels}')\n",
        "\n",
        "# Create binary columns for each disease\n",
        "for c_label in all_labels:\n",
        "    if len(c_label)>1: # leave out empty labels\n",
        "        # Add a column for each disease\n",
        "        data[c_label] = data['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n",
        "\n",
        "print(f\"shape: {data.shape}\")\n",
        "data.head()\n",
        "\n",
        "label_counts = data['Finding Labels'].value_counts()\n",
        "label_counts\n",
        "\n",
        "# Filter out rare disease combinations\n",
        "data = data.groupby('Finding Labels').filter(lambda x : len(x)>11)\n",
        "\n",
        "label_counts = data['Finding Labels'].value_counts()\n",
        "print(label_counts.shape)\n",
        "label_counts\n",
        "\n",
        "# Split data into train/validation/test\n",
        "train_and_valid_df, test_df = train_test_split(data,\n",
        "                                               test_size = 0.20,\n",
        "                                               random_state = 2018,\n",
        "                                              )\n",
        "\n",
        "train_df, valid_df = train_test_split(train_and_valid_df,\n",
        "                                      test_size=0.20,\n",
        "                                      random_state=2018,\n",
        "                                     )\n",
        "\n",
        "print(f'Entrenamiento {train_df.shape[0]} Validación {valid_df.shape[0]} Prueba: {test_df.shape[0]}')\n",
        "\n",
        "# Create data generator\n",
        "base_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "IMG_SIZE = (224, 224)\n",
        "def flow_from_dataframe(image_generator, dataframe, batch_size):\n",
        "\n",
        "    df_gen = image_generator.flow_from_dataframe(dataframe,\n",
        "                                                 x_col='Path',\n",
        "                                                 y_col=all_labels,\n",
        "                                                 target_size=IMG_SIZE,\n",
        "                                                 classes=all_labels,\n",
        "                                                 color_mode='rgb',\n",
        "                                                 class_mode='raw',\n",
        "                                                 shuffle=False,\n",
        "                                                 batch_size=batch_size)\n",
        "\n",
        "    return df_gen\n",
        "\n",
        "train_gen = flow_from_dataframe(image_generator=base_generator,\n",
        "                                dataframe= train_df,\n",
        "                                batch_size = 32)\n",
        "\n",
        "valid_gen = flow_from_dataframe(image_generator=base_generator,\n",
        "                                dataframe=valid_df,\n",
        "                                batch_size = 32)\n",
        "\n",
        "test_gen = flow_from_dataframe(image_generator=base_generator,\n",
        "                               dataframe=test_df,\n",
        "                               batch_size = 32)\n",
        "\n",
        "train_x, train_y = next(train_gen)\n",
        "print(f\"Dimensiones de la imagen: {train_x[1].shape}\")\n",
        "print(f\"Vector de enfermedades: {train_y[1]}\")\n",
        "\n",
        "# Build model\n",
        "input_shape=(224, 224, 3)\n",
        "img_input = Input(shape=input_shape)\n",
        "\n",
        "base_model = EfficientNetB5(include_top=False, input_tensor=img_input, input_shape=input_shape,\n",
        "                         pooling=\"avg\", weights='imagenet')\n",
        "\n",
        "x = base_model.output\n",
        "predictions = Dense(len(all_labels), activation=\"sigmoid\", name=\"predictions\")(x)\n",
        "model = Model(inputs=img_input, outputs=predictions)\n",
        "\n",
        "# Save model summary\n",
        "with open('model_summary.txt', 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary()\n",
        "\n",
        "# Setup callbacks\n",
        "model_train = model\n",
        "output_weights_name='weights.weights.h5'\n",
        "checkpoint = ModelCheckpoint(\n",
        "             output_weights_name,\n",
        "             save_weights_only=True,\n",
        "             save_best_only=True,\n",
        "             verbose=1,\n",
        "            )\n",
        "\n",
        "# Custom AUROC callback\n",
        "class MultipleClassAUROC(Callback):\n",
        "    \"\"\"\n",
        "    Monitor mean AUROC and update model\n",
        "    \"\"\"\n",
        "    def __init__(self, generator, class_names, weights_path, stats=None):\n",
        "        super(Callback, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.class_names = class_names\n",
        "        self.weights_path = weights_path\n",
        "        self.best_weights_path = os.path.join(\n",
        "            os.path.split(weights_path)[0],\n",
        "            f\"best_{os.path.split(weights_path)[1]}\",\n",
        "        )\n",
        "        self.best_auroc_log_path = os.path.join(\n",
        "            os.path.split(weights_path)[0],\n",
        "            \"best_auroc.log\",\n",
        "        )\n",
        "        self.stats_output_path = os.path.join(\n",
        "            os.path.split(weights_path)[0],\n",
        "            \".training_stats.json\"\n",
        "        )\n",
        "        # for resuming previous training\n",
        "        if stats:\n",
        "            self.stats = stats\n",
        "        else:\n",
        "            self.stats = {\"best_mean_auroc\": 0}\n",
        "\n",
        "        # aurocs log\n",
        "        self.aurocs = {}\n",
        "        for c in self.class_names:\n",
        "            self.aurocs[c] = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \"\"\"\n",
        "        Calcula el promedio de las Curvas ROC y guarda el mejor grupo de pesos\n",
        "        de acuerdo a esta metrica\n",
        "        \"\"\"\n",
        "        print(\"\\n*********************************\")\n",
        "        self.stats[\"lr\"] = float(kb.get_value(self.model.optimizer.learning_rate))\n",
        "        print(f\"Learning Rate actual: {self.stats['lr']}\")\n",
        "\n",
        "        \"\"\"\n",
        "        y_hat shape: (#ejemplos, len(etiquetas))\n",
        "        y: [(#ejemplos, 1), (#ejemplos, 1) ... (#ejemplos, 1)]\n",
        "        \"\"\"\n",
        "        y_hat = self.model.predict(self.generator, steps=len(self.generator), verbose=0)\n",
        "        y_hat = y_hat[:len(self.generator.labels)]\n",
        "        y = self.generator.labels\n",
        "\n",
        "        print(f\"*** epoch#{epoch + 1} Curvas ROC Fase Entrenamiento ***\")\n",
        "        current_auroc = []\n",
        "        for i in range(len(self.class_names)):\n",
        "            try:\n",
        "                score = roc_auc_score(y[:, i], y_hat[:, i])\n",
        "            except ValueError:\n",
        "                score = 0\n",
        "            self.aurocs[self.class_names[i]].append(score)\n",
        "            current_auroc.append(score)\n",
        "            print(f\"{i+1}. {self.class_names[i]}: {score}\")\n",
        "        print(\"*********************************\")\n",
        "\n",
        "        mean_auroc = np.mean(current_auroc)\n",
        "        print(f\"Promedio Curvas ROC: {mean_auroc}\")\n",
        "        if mean_auroc > self.stats[\"best_mean_auroc\"]:\n",
        "            print(f\"Actualización del resultado de las Curvas de ROC de: {self.stats['best_mean_auroc']} a {mean_auroc}\")\n",
        "\n",
        "            # 1. copy best model\n",
        "            shutil.copy(self.weights_path, self.best_weights_path)\n",
        "\n",
        "            # 2. update log file\n",
        "            print(f\"Actualización del archivo de logs: {self.best_auroc_log_path}\")\n",
        "            with open(self.best_auroc_log_path, \"a\") as f:\n",
        "                f.write(f\"(epoch#{epoch + 1}) auroc: {mean_auroc}, lr: {self.stats['lr']}\\n\")\n",
        "\n",
        "            # 3. write stats output, this is used for resuming the training\n",
        "            with open(self.stats_output_path, 'w') as f:\n",
        "                json.dump(self.stats, f)\n",
        "\n",
        "            print(f\"Actualización del grupo de pesos: {self.weights_path} -> {self.best_weights_path}\")\n",
        "            self.stats[\"best_mean_auroc\"] = mean_auroc\n",
        "            print(\"*********************************\")\n",
        "        return\n",
        "\n",
        "training_stats = {}\n",
        "auroc = MultipleClassAUROC(\n",
        "    generator=valid_gen,\n",
        "    class_names=all_labels,\n",
        "    weights_path=output_weights_name,\n",
        "    stats=training_stats\n",
        ")\n",
        "\n",
        "# Compile model\n",
        "initial_learning_rate=1e-3\n",
        "optimizer = Adam(learning_rate=initial_learning_rate)  # Changed from 'lr' to 'learning_rate'\n",
        "model_train.compile(optimizer=optimizer, loss=\"binary_crossentropy\")\n",
        "\n",
        "# Setup all callbacks\n",
        "logs_base_dir = '/kaggle/working/'\n",
        "patience_reduce_lr=2\n",
        "min_lr=1e-8\n",
        "callbacks = [\n",
        "            checkpoint,\n",
        "            TensorBoard(log_dir=os.path.join(logs_base_dir, \"logs\")),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience_reduce_lr,\n",
        "                              verbose=1, mode=\"min\", min_lr=min_lr),\n",
        "            auroc,\n",
        "        ]\n",
        "\n",
        "# Train model\n",
        "epochs=1\n",
        "fit_history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=train_gen.n//train_gen.batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=valid_gen,\n",
        "    validation_steps=valid_gen.n//valid_gen.batch_size,\n",
        "    callbacks=callbacks,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(1, figsize = (15,8))\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.plot(fit_history.history['loss'])\n",
        "plt.plot(fit_history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Load best weights and evaluate\n",
        "model.load_weights('weights.weights.h5')\n",
        "\n",
        "test_gen.reset()\n",
        "pred_y = model.predict(test_gen, steps=len(test_gen), verbose=1)\n",
        "\n",
        "# Trim predictions to match actual label count (handles incomplete last batch)\n",
        "pred_y = pred_y[:len(test_gen.labels)]\n",
        "\n",
        "test_gen.reset()\n",
        "test_x, test_y = next(test_gen)\n",
        "print(f\"Vector de enfermedades: {test_y[1]}\")\n",
        "print(f\"Vector de enfermedades producto de la predicción: {pred_y[2]}\")\n",
        "\n",
        "# Plot ROC curves\n",
        "test_gen.reset()\n",
        "test_x, test_y = next(test_gen)\n",
        "# Space\n",
        "fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n",
        "for (idx, c_label) in enumerate(all_labels):\n",
        "    #Points to graph\n",
        "    fpr, tpr, thresholds = roc_curve(test_gen.labels[:,idx].astype(int), pred_y[:,idx])\n",
        "    c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n",
        "\n",
        "#convention\n",
        "c_ax.legend()\n",
        "\n",
        "#Labels\n",
        "c_ax.set_xlabel('False Positive Rate')\n",
        "c_ax.set_ylabel('True Positive Rate')\n",
        "\n",
        "# Save as a png\n",
        "fig.savefig('barely_trained_net.png')\n",
        "\n",
        "# Calculate overall ROC AUC\n",
        "auc_score = roc_auc_score(test_gen.labels, pred_y)\n",
        "print('ROC AUC: %f' % auc_score)\n",
        "\n",
        "# Visualize predictions\n",
        "sickest_idx = np.argsort(np.sum(test_y, 1)<1)\n",
        "\n",
        "#Space of images\n",
        "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
        "\n",
        "# Padding\n",
        "fig.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
        "counter = 0\n",
        "\n",
        "for (idx, c_ax) in zip(sickest_idx, m_axs.flatten()):\n",
        "\n",
        "    # Image show\n",
        "    c_ax.imshow(test_x[idx, :,:,0], cmap = 'bone')\n",
        "\n",
        "    stat_str = [n_class[:4] for n_class, n_score in zip(all_labels, test_y[idx]) if n_score>0.5]\n",
        "\n",
        "    # Building the labels\n",
        "    pred_str = [f'{n_class[:4]}:{p_score*100:.2f}%'\n",
        "                for n_class, n_score, p_score\n",
        "                in zip(all_labels,test_y[idx],pred_y[idx])\n",
        "                if (n_score>0.5) or (p_score>0.5)]\n",
        "\n",
        "    c_ax.set_title(f'Index {idx}, Labels: '+', '.join(stat_str)+'\\n Pred: '+', '.join(pred_str))\n",
        "    c_ax.axis('off')\n",
        "fig.savefig('trained_img_predictions.png')\n",
        "\n",
        "print(\"\\nTraining and evaluation complete!\")\n",
        "print(\"Output files saved to /kaggle/working/\")"
      ],
      "metadata": {
        "_uuid": "4730687d-5a3a-4a55-bc25-06efffc8a963",
        "_cell_guid": "a0f38d69-4b78-4134-8c29-34bef3b5a5f4",
        "trusted": true,
        "jupyter": {
          "outputs_hidden": false
        },
        "execution": {
          "iopub.status.busy": "2025-12-09T12:21:34.024064Z",
          "iopub.execute_input": "2025-12-09T12:21:34.024872Z",
          "iopub.status.idle": "2025-12-09T12:23:12.789595Z",
          "shell.execute_reply.started": "2025-12-09T12:21:34.024845Z",
          "shell.execute_reply": "2025-12-09T12:23:12.78868Z"
        },
        "id": "2g_VqZYRKsl5"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}